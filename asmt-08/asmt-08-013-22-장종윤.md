---
title: 013-22 장종윤 (과제-08)
layout: home
nav_order: 22
parent: 과제-08 기말과제 초고 작성하기
permalink: /asmt-08/013-22
---

# (초고) 과제-08 기말과제 초고 작성하기 013-22 장종윤 

# 제목: AI 비이해성 아래에서의 과학 이론의 신뢰 기준

## 1. 서론
최근 AI는 놀라운 성장세를 보여주고 있다. 구글의 Deepmind에서 개발한 AlphaFold는 지금도 새로운 단백질 구조를 예측하고 있고, Automation Lab에서는 재료의 레시피를 연구한다. 이제 AI는 산업의 혁신을 넘어서서, 인류 지성의 결정체인 과학 분야에서도 지형을 뒤흔들고 있다. 그러나 이러한 AI는 내부 구조를 인간이 이해하고 해석하기 어렵다는 점에서 치명적이다. 예컨대 이런 상황을 생각할 수 있다. 한 연구실에서 새로운 재료의 특성을 연구하고자 한다. AI는 이전에 수행되었던 비슷한 재료의 시험 데이터를 바탕으로, 과학자들의 상식에는 맞지 않은 실험 조건을 제안했다. 반면, 과학자는 열역학, 유체역학을 바탕으로 상식적인 조건을 제안하지만, 비슷한 재료 조합에서는 실험 결과가 없다. 예산의 제약으로 당장은 한 번만 실험할 수 있을 때, 어느 쪽 설계를 선택해야 할까? 

이는 과학이 이론을 정당화해 온 방식 자체에 의문을 던지는 일이다. 기존 과학 철학에서는 이를 과학과 비과학을 구분하는 구획 기준으로 오랜 기간 다루어 왔다. 논리실증주의, 반증주의, 패러다임 이론과 연구 프로그램 이론은 상이한 맥락에서 각각 기준을 제시해왔다. 그러나 이 기준들이 모두 이론의 이해 가능성을 일정 부분 전제했다는 점에서 공통적인 한계를 갖는다. AI의 내부가 이해 불가능한 상황에서 과학철학의 기준을 그대로 적용할 수는 없는 노릇이다. 이에 대해 이해 가능성을 강화하거나 배제하려는 XAI와 성능 우선주의의 시도가 이어지고 있지만, 본 논증문에서는 조금 다른 접근을 하고자 한다. 

본 논증문은 내부 구조를 이해하지 못하더라도 과학 이론을 승인할 수 있는 최소 조건이 있다고 주장한다. 논제는 다음과 같다. AI 이론이 이해 불가능한 상황에도 불구하도, 경험적 신뢰도(E)와 지식 정합성(K)으로 구성된 필요 조건을 충족하면, 그 이론은 과학 이론으로 승인될 자격을 갖춘다. 이때, 승인은 후속 연구의 설계나 수행의 근거로 사용될 수 있는 상태를 의미한다. 

본론은 다음과 같은 구조를 따른다. 2, 3절에서는 기존 승인 기준이 어떤 한계를 갖는지 제시하고, AI의 블랙박스가 야기하는 다섯 가지 난점을 판정 불가능성으로 구체화한다. 다음으로 4절에서는 구체적인 필요조건을 제시하고, 이어서 5절에서 이 조건의 역할을 논한다. 6절은 실제 사례를 살핀다. 마지막으로 7절에서 경쟁 관계에 있는 판정 규범과 관련하여 반론과 재반론을 검토한다. 

## 2. 기존의 과학 철학 모델과 그 한계
기존에 다양하게 논의되던 이론 승인 모델은 크게 논리실증주의, 포퍼의 반증주의, 쿤의 패러다임 이론, 라카토시의 연구 프로그램 이론의 전통적인 구획 기준과 견고성 이론, 신뢰성 이론으로 대표되는 현대의 기준으로 분류된다. 이들은 출발점은 다르지만 모두 일정 수준의 내부 이해 가능성을 전제한다는 공통점을 갖는다. 즉, 이론이 어떻게 작동하는지 파악이 가능해야 관찰, 반증, 진보성, 일관성과 같은 외부 기준도 의미를 가지지만, AI 비이해성은 이 전제를 직접적으로 약화시킨다. 

먼저 논리실증주의에서는 관찰이 이론으로부터 도출되고 다시 이론으로 환원되는 정당화 구조를 강조한다. 반면, 포퍼의 반증주의(Popper 1959/2002)는 관찰이 이론으로 환원되는 과정에서 발생하는 귀납의 한계를 지적했고, 이에 대한 해법으로 반증에 집중할 것을 제시했다. 쿤의 패러다임 이론과 라카토시의 연구 프로그램 이론은 과학자 공동체를 포함하는 정상과학 혹은 연구 프로그램 단위를 세워 규범적 기준의 가능성을 제시했다. 그러나 AI 이론의 해석 불가능성은 이들 기준을 약화한다. 우선 이론이 인간의 언어로 해체되고 다시 서술되지 못한다면 관찰을 어떻게 이론으로 환원하는 가? 무엇에 대한 반증인 가? 성능 우선주의로 귀결되지 않는 구체적인 판정 규칙이 존재하는 가? 이에 대해 현대의 견고성 이론과 신뢰성 이론(Beisbart and Norton 2012; Malinsky 2023)은 내부의 이해 없이도 반복적인 패턴을 근거로 신뢰성을 판단할 수 있다는 통찰을 제공하지만, 반복 패턴이 ‘무엇의 반복인지’를 해석하기 위해서는 여전히 이해 가능성을 완전히 포기하지는 못한다. 

결국, 기존의 이론 승인 모델들은 서로 다른 방식으로 기준을 제시했지만, 모두 일정 수준의 내부 이해 가능성을 전제한다는 점에서 AI의 블랙박스 앞에서는 공통된 한계를 드러낸다. 이에 따라, 이해 불가능한 상황에서도 작동할 수 있는 새로운 구획 기준이 요구된다.  

## 3. 블랙박스가 야기하는 난점과 판정 불가능성
AI의 블랙박스는 이론의 학습과 수정에서 여러 문제를 야기한다. 학습시에는 재현성의 붕괴, 편향의 고착, 우연적 적합의 고착이 이론을 판정하기 어렵게 만들고, 수정의 불투명성이 이론의 누적적 진보를 방해한다. 그리고 이들이 종합되어 경쟁 평가의 약화로 드러나고, 이들 난점은 이론에 대한 각기 다른 판정 불가능성으로 이어진다. 

먼저 재현성의 붕괴(Munafò et al. 2017), 편향의 고착, 우연적 적합(Schmidt and Lipson 2009)의 고착은 모두 AI가 이론을 생성하는 과정에서 발생하는 난점들이다. 이들은 각각 학습의 부수적인 요인, 훈련 데이터의 편향, 과도한 모델 자유도로 인한 과대 적합에 의해 관찰된다. 우선 구체적으로 AI는 동일한 모델과 데이터셋을 사용하더라도 초기 조건, 학습률, 최적화 알고리즘, 전처리 방식과 같은 요인에 의해 결과가 변한다. 이는 동일한 AI를 재현하는 데에 큰 장애물이 된다. 다음으로 훈련 데이터의 편향은 데이터가 통계적 무작위성을 보장받지 못할 때에 발생한다. 가령 모든 성인에 대한 가설을 세우고서 서울대학교 학생만을 대상으로 인터뷰를 진행하면, 해당 인터뷰가 모든 성인데 대한 대표성을 상실하는 것과 같은 이치이다. 비슷하게 과도한 모델 자유도로 인한 과대 적합은 매우 복잡한 AI 모델을 사용했지만 훈련 데이터셋이 상대적으로 작은 경우 자주 발생한다. 이는 AI가 어떤 전체적인 경향성을 포착하지 않고 개별 요소를 하나하나 표현하는 게 가능해져 발생한다. 예를 들어, 서울대학교 학생을 대상으로 한 가설에서 AI가 모든 서울대학교 학생을 구분하여 평가하는 경우 경향성은 드러나지 않는다. 

이러한 학습에서의 난점은 구해낸 이론이 성공하는 이유와 실패하는 이유 모두에서 교란을 준다. 재현이 실패한 상황에서, 그 실패의 원인이 실제 세계에 그런 규칙이 없어서인지, 그저 AI가 부수적 요인에 의해 학습에 실패한 것인지 알 수가 없다. 또 성공한 것처럼 보이는 AI 이론도 데이터 편향이나 과대적합의 결과인지 일반적 구조를 포착한 것인지 구분하기 어렵다. 더욱이 더 넓은 영역으로의 확장을 시도할 때 급격한 정확도의 붕괴가 발생할 가능성도 존재한다. 기존의 구획 기준에서는 이론을 핵심 가설, 보조 가정, 암묵지로 해체하고 이를 바꾸어 가며 다양한 조합을 시도할 수 있다. 이로써 부수적 요인을 배제하고, 데이터 편향이 어떻게 내제되는지를 추적할 수 있으며, 베컴의 면도날과 같은 외재적 가치에 의해 과대적합도 대체로 배제된다. 그러나 AI의 블랙박스 상황에서는 이론의 안정성, 일반성, 구조적 타당성에 대한 판정 불능으로 이어진다. 

더욱이 학습시의 판정 불능에 더해서 AI 모델은 수정 과정에서도 불투명성이 나타난다. 기존 구획 기준에서는 이론을 해체하고 개별 요소를 조정하면서 어떤 수정이 문제를 해결했는지를 비교적 명확하게 추적할 수 있었다. 반면, AI 모델에서의 수정은 대개 재학습이나 모델의 구성을 변경하면서 이루어지기에 전체 구조에 동시에 영향을 미친다. 그리고 내부 규칙을 해석할 수 없는 상황과 겹치면, 어떤 변경이 실제로 문제 해결에 기여했는지, 수정이 이론의 설명 범위를 확장하는 진보적 변화인지, 아니면 단순한 파라미터 튜닝인지 판별하기 어렵다. 이는 곧 이론의 진보 여부에 대한 판정 불가능성으로 연결된다. 새로운 예측, 더 넓은 도메인, 기존 문제 해결과 같은 라카토시의 진보적 문제 이동은 수정 전후의 구조적 차이를 분석함으로써 판정된다. 그러나 블랙박스는 매 학습마다 이론의 변경 사항을 개별적으로 추적할 수 없고, 문제가 되는 영역뿐 아니라 기존에 성공적이었던 영역까지도 재검토해야 하는, 사실상 누적적이지도 연속적이지도 않는, 복권에 당첨되기를 기도하는 연구가 반복된다. 

마지막으로 경쟁 이론 간 비교 평가의 약화도 있다. 기존에는 과학 이론을 설명력, 구조적 단순성, 이론적 일관성, 예측 범위와 같은 다양한 기준에 따라 서로 비교해왔다. 하지만 AI 기반 이론에서는 내부 구조가 해석 불가능할 경우, 이러한 기준 중 상당수가 직접 적용될 수 없다. 이때, 비교 가능한 것은 주로 성능 지표 하나이거나 제한된 벤치마크 환경에서의 점수 밖에 없다. 이로 인해 경쟁 이론 간의 상대적 우월성에 대한 판정 불가능성으로 귀결된다. 두 모델이 도출하는 각각의 이론이 있을 때, 어느 쪽이 더 우수한 이론인지 판정하려면 단순 점수 비교를 넘어 누가 더 넓은 도메인을 다루는지, 누가 더 안정적이고 견고한 패턴을 포착하는지, 누가 기존 이론 체계와 더 잘 통합되는지를 종합적으로 평가해야 한다. 그러나 이해 불능성은 이러한 총체적인 비교 가능성을 축소시키며, 단순 성능주의로 귀결될 위협을 갖는다. 

따라서, AI의 블랙박스는 학습과 수정, 대안 이론과의 경쟁 상황에서 각기 다른 난점을 갖고, 이론 평가의 여러 지점에 균열을 낸다. 그리고 곧 안정성, 일반성, 구조적 타당성, 진보 여부, 상대적 우월성 판정을 각각 불가능하거나 어렵게 만든다. 이러한 판정 불가능성들은 부분적으로 중첩되고 증폭되면서 이론 승인 과정을 전반적으로 불투명하게 만든다. 

## 4. E/K 규범
E/K 규범은 경험적 신뢰도와 지식 정합성이라는 용어로 대표되며 재현성, 일관성, 경쟁성을 포함하는 이론 승인의 최소 필요 조건이다. 이 조건은 내부 규칙의 이해 없이도 이론으로의 승인을 난처하게 하던 판정 불가능성을 우회적으로 복원한다. 

E 기준은 이론이 실제 세계에서 얼마나 안정적으로 기능하는지 평가하며 기초적 안정성, 국소적 견고성, 광역적 재현성, 장기적 진보성을 포함한다. 먼저 기초적 안정성(E1)은 다양한 조건에서의 예측 안정성을 요구한다. 단순한 정확도를 넘어서서 환경과 조건의 변화에도 유지되는지를 확인한다. 국소적 견고성(E2)은 초기값과 전처리 등의 부수적인 요소의 변동에도 핵심 결과가 유지되는지를 점검한다. 이때, E1은 이론이 산출하는 관찰명제의 참 여부를 통해 판정되고, E2는 이론의 학습에서의 견고성을 판정한다는 점에서 비슷하지만 다른 층위를 나타낸다. 광역적 재현성(E3)은 서로 다른 연구 집단, 장비, 데이터 조건에서 동일 결과가 반복되는지를 확인한다. 마지막으로 장기적 진보성(E4)은 시간이 지남에 따라 이론이 성능이 단순 조정이 아니라 도메인 확장과 문제 해결 향상으로 이어지는지 평가한다. 

K 기준은 이론이 기존 지식 체계와 맺는 관계를 평가한다. K1은 관찰 명제와의 비모순성이다. 이때, E1과 다른 점은 판정의 재료가 되는 관찰명제가 경험적으로 참이라는 점보다는 기존 이론체계에서 널리 참으로 인정받았다는 점에 집중한다는 부분이다. K1은 해당 이론이 예측하는 도메인에서 산출하는 관찰명제의 참 여부보다 기존에 사실로 인정받는 관찰명제와의 관계에 대한 진술이다. K2는 기존 핵심 이론과의 긴장이 허용되지만 전체 체계를 붕괴시키는 급진적 모순은 금지하여 경쟁 가능한 지위인지를 본다. K3는 대안 이론보다 더 많은 참인 예측, 넓은 설명 범위를 제공하는지를 확인해 경쟁적 우월성을 판정한다. K2와 K3는 기존 이론이나 대안 이론 간의 경쟁적 지위가 어떤 조건에서 형성되는지 나타낸다. K4는 이론이 기존 체계 내에서 부분적으로 통합될 수 있는지를 평가해 지식의 누적적 연속성을 보존한다. 

예를 들어, 신약 후보의 간 독성을 예측하는 AI 기반 이론 Tx를 생각해 보자. Tx는 환자의 기본 특성, 혈중 농도, 투여 스케줄 등을 입력으로 받아 일정 기간 내 심각한 간 손상 발생 확률을 출력한다. 여기서 관찰 명제는 “투여 후 30일 내 특정 성분의 수치가 기준 이상으로 상승했다”, “입원이 필요하지 않았다”와 같은 임상 지표에 대한 진술이고, Tx의 내부 규칙은 심층 신경망 구조 때문에 해석하기 어렵다. 이 경우 E 기준은 다음과 같이 적용될 수 있다. E1은 동일 병원 내에서 서로 다른 환자 집단에 대해 기본적인 예측 안정성이 유지되는지를 요구한다. E2는 전처리 방식이나 초기 파라미터 설정을 조금 달리해도, 어떤 환자를 고위험으로 분류하는지가 크게 달라지지 않는지를 통해 국소적 견고성을 점검한다. E3는 서로 다른 병원, 국가, 인구 구성에서 독립적으로 수집된 데이터에 Tx를 적용했을 때 유사한 수준의 성능이 반복되는지를 보는 광역적 재현성 기준이다. 마지막으로 E4는 시간이 지나 새로운 약물과 환자군이 포함되었을 때도 성능이 유지되거나 향상되는지를 통해, 단순 과적합이 아닌 장기적 진보성을 평가한다. 

동시에 K 기준은 Tx가 기존 지식 체계와 맺는 관계를 점검한다. K1은 “고용량, 장기 복용이 간 손상 위험을 증가시킨다”와 같은 반복적으로 검증된 독성학 관찰 명제와 노골적인 충돌이 없는지를 요구한다. K2는 일부 결과가 기존 약동, 약력학 이론과 긴장을 형성할 수는 있지만, 전체 독성학 이론을 폐기해야 할 정도의 급진적 모순을 야기하지 않는지를 본다. 나아가 Tx가 기존 독성 위험 점수보다 더 넓은 환자군에서 더 정확한 예측을 제공한다면 K3, 기존 위험 분류 체계에 통합 가능한 형태로 활용될 수 있다면 K4를 부분적으로 충족했다고 볼 수 있다. 이와 같이 Tx의 내부 메커니즘을 이해할 수 없더라도, 다양한 조건에서의 반복적 성능과 기존 관찰, 이론과의 정합성을 통해 이 이론을 어느 수준까지 승인할 수 있는지 판단할 수 있다. 

마지막으로 E/K 규범의 철학적 기반은 여러 전통적 논의에 걸쳐 있다. 라카토시(Lakatos 1978)의 진보성 개념은 E4와 K3의 판단 논리를 뒷받침하며, 견고성 이론(Beisbart and Norton 2012; Munafò et al. 2017)은 E2와 E3의 외부적 반복성 판단을 정당화한다. 신뢰성 이론은 내부 이해 없이도 외부 패턴이 안정적으로 반복되면 승인할 수 있다는 점을 강조하며 E1~E3의 핵심 기반이 된다. 라우던(Laudan 1977)의 부분적 연속성 논의는 K1, K2, K4의 통합 구조를 지지한다. 종합하면, E/K 규범은 경험적 안정성과 이론적 정합성을 통해 AI 기반 이론의 승인 여부를 평가하는 최소 필요조건을 구성한다. 내부 이해가 부족하더라도 이 두 축을 충족하면 승인의 최소 자격을 갖추며, 둘 중 하나라도 결여되면 승인 대상에서 제외된다. E/K는 비이해성의 시대에서 기존 과학적 가치를 유지하면서 승인 절차를 합리적으로 재구성하려는 규범적 틀이다. 

## 5. E/K 기준이 난점을 해소하는 방식
AI의 블랙박스는 안정성, 일반성, 구조적 타당성, 진보 여부, 경쟁 우월성이라는 핵심 판단 영역을 차례로 불투명하게 만든다. 이에 대해 E/K 규범은 판정 불가능성을 외부 기준을 통해 우회적으로 복원하는 방식으로 작동한다. 즉, 내부 구조는 이해할 수 없지만 외부에서 반복적으로 정합적이며, 경쟁에서 우월한 점을 확인함으로써 승인 판단을 다시 가능하게 만든다. 이때 규범은 해당 AI 이론의 존재성을 보장하기보다는 필터와 같이 기능한다. 

규칙의 안정성, 일반성, 구조적 타당성에 대한 판정 불가능성은 반복성을 중심으로 정합성을 확인하여 복원한다. 즉, 부수적인 구성 요소, 데이터 자체의 편향, 모델의 높은 자유도로 인해 실제로는 성공해서는 안 되지만 성공한 것처럼 보이는 이론들을 다양한 조건에서 반복적으로 시험하여 걸러 낸다. 이를 위해 E1을 기반으로 E2, E3, E4, K1이 집중적으로 적용되며 E4, K2, K3가 간접적으로 보조한다. 

이 방법은 장기적으로 보았을 때에는 무엇을 승인하고, 폐기하는지 사후적 판정에서는 매우 유용하다. 다만, 이를 실무적으로 유용하게 사용하기 위해서는 연구 시에 적용할 수 있는 단기와 중기에서의 규범으로 확장되어야 한다. 이에 대해, 각 규범이 집중적으로 판정되는 시간 축을 구분해야 한다. 먼저, 단기에서는 E1과 E2가 중심이 된다. 기초적인 안정성이 유지되는지 국소적 영역에서 견고한 지가 확인되면 더 넓은 영역에서, 다양한 조건에서 시험될 최소 조건을 만족한다. 중기에서는 E3와 K1, K2가 중요해진다. 서로 다른 연구 집단, 장비, 데이터 조건에서도 동일한 결과가 반복되는지, 이미 확립된 관찰 명제와 충돌하지 않는지, 기존 핵심 이론과 조정 가능한 관계를 형성할 수 있는지가 판단 기준이다. 이 단계에서 이론은 단일 실험실 수준을 넘어 더 넓은 지식 체계에서 일반성을 주장할 수 있는 기반을 확보한다. 장기에서는 E4, K3, K4를 중심으로 보편적인 영역으로 적용되는 지, 대안 이론과의 경쟁에서 승리하거나 적어도 보류되고 유지될 수 있는지가 중점이 된다. 이 과정의 끝에서 이론은 단순 도구를 넘어 후속 연구의 인프라로 편입될 수 있는 강한 승인 수준을 갖추게 된다. 

반면, 수정 과정이 불투명한 경우에는 이야기가 조금 다르다. 성능 변화와 이론 체계 내의 위치 변화라는 두 경로로 약하게 복원되지만 근본적인 보장과는 거리가 있다. E4는 수정 후 성능 변화를 도메인 확장, 문제 해결 능력의 증가를 기준으로 먼저 판정하고, K4는 수정된 이론이 기존 이론체계 속에서 유지될 수 있는지를 확인한다. 즉, 기존 도메인과 새로운 도메인에서 진보하는지 판정할 수 있다. 다만, 진보에서 기존 영역의 성공을 다음 이론에도 이어가는 누적성은 보장하지 못하고, 승인 가능한 최소한의 필터로서만 기능한다. 

끝으로, 경쟁 우월성 판정이 불투명해질 때는, 외부적 비교 기준을 이용한다. K3는 이론이 대안 이론보다 참 예측과 설명 범위에서 우월한지 평가하고, K4는 이론 체계 내의 지위를 점검해 단순 성능 점수 이상의 체계적 우월성을 판단할 수 있게 한다. 이는 장기 시점에서의 판단의 기초가 된다.  요약하면, E/K 규범은 블랙박스가 야기하는 난점을 해결하는 것이 아니라, 그 난점들이 만든 판정 불가능성 영역을 반복성, 정합성, 경쟁성이라는 외부적 기준을 통해 판정 가능한 상태로 복원한다. 이는 내부 메커니즘에 접근할 수 없는 비이해성 시대에도 승인 판단을 유지하기 위한 최소한의 절차적 장치를 제공한다. 

## 6. 사례들
실제 사례에서도 이러한 구조는 관찰된다. 단백질 구조 예측 모델 AlphaFold는 내부 메커니즘이 불투명함에도 다양한 조건에서의 안정적 성능, 다수 연구 집단의 독립 재현성, 기존 생물물리학 관찰 및 이론과의 정합성을 충족했다. 장기적으로는 많은 대안 모델보다 우월한 예측 성능을 보여 경쟁적 지위를 획득했다(Jumper 2021). 자율 실험실 사례에서도 규범이 충족되는 방식이 드러난다. 여러 실험 라인에서 동일한 패턴이 반복적으로 산출되며 독립 재현성을 확보하고, 장기적으로 탐색 효율이 향상되는 진보성을 보이며, 기존 화학 이론과 재료 이론과 조정 가능한 방식으로 통합되어 강한 승인 가능성을 확보한다(Szymanski 2023). 

반면, Google Flu Trends는 견고성, 재현성, 도메인 확장 기준이 모두 무너져 실패했다. 전년도의 성능을 다음 해에 그대로 재현하지 못했고, 특정 검색어 패턴의 편향이 누적되었으며, 일시적 상관 패턴을 과학적 구조로 오인했다. 결과적으로 검색어 패턴이 외생적 요인에 의해 요동쳤고, 실제 유행 데이터를 과대 추정했으며, 내부 메커니즘의 불투명성으로 적절한 수정이 행해지지 못했다. 이로써 E 규범에서의 붕괴를 겪었으며 좋은 성과를 거두지 못했다. 비슷하게 IBM Watson 사례는 K 기준에서 전문가 지식과의 충돌로 실패했다. 해당 AI는 전문가의 가이드라인과 충돌하는 추천이 빈번했다. 중환자 처방에서 부적절한 권고를 내렸고, 성능도 병원마다 달라지며 범위 확장에도 실패했다. 동일 상황에서도 일관되지 않은 추천이 이어지며 재현성과 범위 확장에도 성공적이지는 못했다. 결국 과학적 승인뿐 아니라 임상적 승인에도 규범의 실패가 모델의 실패로 이어한다. 

종합하면, 이 규범은 실제 연구 환경에서 이미 부분적으로 적용되었다고 볼 수 있다. 해당 규범은 비이해성의 시대에서 승인 판단을 위한 균형적이고 점진적인 체계로 기능한다. 다만, 위 사례들은 모두 단일한 AI 시스템을 사후적으로 평가한 결과이기 때문에 얼핏 당연한 결론처럼 보일 수도 있다. 그러나 다양한 AI 이론이 난립하는 상황에서는 이러한 E/K 규범이 단순한 사후 평가를 넘어 승인 판단을 선별하는 규범적 필터로 기능한다. 

## 7. 대안 규범에 대한 반론
E/K 규범은 다음의 대표적인 두 가지 대안 규범과 경쟁하게 된다. 첫 번째는 설명 가능성을 핵심 기준으로 삼는 XAI (Explainable AI)이고, 두 번째는 단일 벤치마크 성능을 최우선 기준으로 삼는 성능 중심의 규범이다. 이에 대해 E/K 규범은 이 두 규범을 대체하려는 것이 아니라, 설명성 확보가 어렵고 단일 성능 지표가 일반성을 대변하기 어려운 환경에서 작동하는 보충적 규범이라는 점을 밝힌다. 

먼저, XAI는 충분한 수준의 이해가능성 없이는 복잡한 모델을 사용해서는 안 된다는 입장을 취한다. 이 관점에서 볼 때, E/K 규범은 내부 구조를 이해하지 못한 상태에서 단지 다양한 환경에서의 반복적 테스트만을 기준으로 하는 경험적 생존 규범에 불과하다. XAI는 편향이 내부에서 어떻게 발생하고 축적되는지 추적할 수 없다면 근본적인 안전을 보장하지 못하며 승인되어서는 안 된다고 비판한다. 그러나 설명성이 이상적 기준이라는 점에는 이견이 없지만, 현재의 대규모 딥러닝 모델은 충분한 수준의 설명성이 무엇인지조차 합의가 어려우며, 설명 가능한 수준의 단순화를 수행할 경우 원래의 성능과 특성이 상당 부분 손실되는 문제가 있다. 더 나아가, 설명성을 판단하는 기준 자체가 가치 판단을 요구하기 때문에 새로운 규범 논쟁을 야기한다. 이와 같은 이유로 실제 정책, 실무 환경에서는 설명성이 확보될 때까지 AI의 사용을 보류하는 것이 사실상 불가능하다. 그러므로 E/K 규범은 설명성 기반 판단이 작동하지 못하는 공백 구간에서 최소한의 안정성을 보장하기 위한 보충 규범으로 이해되어야 한다. 설명 가능성이 확보되는 영역에서는 XAI 규범을 우선하지만 그렇지 못하는 경우에는 E/K 규범은 부분적이나마 합리적 선택을 가능하게 하는 현실적 기준을 제공한다. 

다음으로, 산업계나 실무 환경에서는 단일 벤치마크에서 최고 성능을 보이는 모델을 선택하는 성능 우선 규범이 작동한다. 이 규범은 효율성과 명료성을 갖추고 있으며, E/K 규범처럼 다양한 환경을 설계하고 테스트하는 과정은 비용 대비 효용이 낮다고 평한다. 그러나 앞서 밝힌 바와 같이, 학습에서의 편향과 과적합은 현재의 높은 성능을 일반적 조건으로 확장하는 과정에서 붕괴된다. 단일 성능 지표에 의존하는 규범은 특정 분포에 특화된 모델을 보편적인 기준으로 오인할 위험을 방치하며, 이로 인해 실제 적용 단계에서 대규모 실패가 발생할 수 있다. 반면, E/K 규범은 다양한 조건, 다양한 데이터 분포, 다양한 개입 시나리오에 걸친 환경적 성능을 기준으로 일반성을 평가한다. 이는 단일 최적성 기준보다 비용이 더 들 수 있지만, 분포 이동이나 환경 변화가 구조적으로 반복되는 AI 응용 영역에서는 장기적 실패 비용이 훨씬 크다는 점에서 합리적이다. E/K 규범은 성능 중심 규범이 가진 단기 최적화의 위험을 줄이고, 무너지지 않는 성능을 우선하는 방식으로 일반성의 추정을 안정화한다. 

마지막으로 일부 독자는 E/K 규범이 결국 여러 모델을 시험해 보고 덜 나쁜 모델을 선택하자는 수준의 약한 규범이 아니냐고 비판할 수 있다. 그러나 이 규범의 목적은 참된 이론을 식별하는 것이 아니라, 해석 불가능성과 분포 불확실성이 일상화된 환경에서 상대적으로 덜 위험한 선택을 제도화하는 것이다. 이는 경제학과 정책학에서도 흔히 채택되는 차선 이론과 유사한 구조를 가지며, 실제 세계의 제약을 전제할 때 오히려 강점이 된다. E/K 규범은 강한 이상 규범들이 작동하지 못하는 조건에서 현실적인 의사결정을 가능하게 한다는 점에서, 약한 규범이 아니라 오히려 실현 가능한 최선의 규범에 가깝다. 

결국, E/K 규범은 XAI 규범이나 성능 중심 규범을 완전히 대체하려는 것은 아니다. 설명성이 확보되는 경우에는 설명성 규범이 우선하고, 단일 벤치마크가 충분한 일반성의 신호로 기능하는 단순한 상황에서는 성능 중심 규범이 효율적이다. 그러나 설명성 확보가 불가능하고 단일 성능이 일반성을 대변할 수 없는 복잡한 환경에서는, E/K 규범이 이러한 두 규범의 공백을 메우는 보충 규범으로 기능한다. 이로써 E/K 규범은 기술적, 철학적 불확실성이 중첩된 조건에서 부분적이지만 실리를 확보하는 수단으로 이해될 수 있다. 

## 8. 결론
AI 비이해성은 단지 이해하기 어려운 새로운 도구를 하나 더 얻었다는 의미가 아니라, 현대 과학에서 이론 정당화가 의존해 온 암묵적 전제들을 다시 묻도록 만든다. 이 논문이 주장한 핵심은, 심층 학습 모델과 같이 내부 구조를 이해하기 어려운 AI 기반 이론에 대해서도, “내부를 이해하는가”라는 기준만으로 승인 여부를 결정할 수는 없으며, 그 대신 경험적 신뢰도(E)와 지식 정합성(K)을 중심으로 한 외부주의적 승인 규범이 최소한의 판단 틀로 요구된다는 점이다. 다시 말해, 비이해성 아래에서 이론 승인 문제는 “이론이 참인가?”라는 전통적 질문이 아니라 “어떤 경험적, 체계적 조건 아래에서 이론을 과학 공동체의 토대로 삼는 것이 허용 가능한가?”라는 규범적 질문으로 재구성되어야 한다. 

E/K 규범의 특징은 비이해성에서 비롯된 난점을 하나의 원리로 해소하려 하기보다, 그것들이 만들어내는 판정 불가능성의 층위를 분리해 각기 다른 축에서 대응한다는 데 있다. 재현성 붕괴, 데이터 편향, 우연적 적합, 수정 불투명성, 경쟁 평가 약화는 모두 내부 구조에 대한 이해 부족에서 비롯되지만, 우리가 실제로 필요로 하는 것은 각각 안정성, 일반성, 구조적 타당성, 진보 여부, 경쟁 우월성에 대한 실천적 판단 기준이다. E 기준은 다양한 조건과 시간 축에서 반복되는 성능 패턴을 통해 이러한 판단을 부분적으로 복원하며, K 기준은 관찰 명제, 핵심 이론, 경쟁 이론과의 관계를 통해 이론의 지식 체계 내 위치를 정돈한다. 이때 승인은 진리의 최종 판정이라기보다, 비이해성 속에서도 “어디까지 신뢰하고 어떻게 사용할 것인지”를 정하는 승인으로 이해된다. 

이 규범은 과학철학 내부의 논의에만 머무르지 않고, 실제 과학 실천과 제도 설계에 대한 함의를 갖는다. E/K 규범을 일관되게 적용한다면, 단일 벤치마크에서의 성능만으로 AI 이론을 빠르게 채택하는 관행은 정당화되기 어렵고, 다양한 환경에서의 재현성 프로토콜, 독립 검증 센터, 장기적인 성능 추적 체계가 승인 절차의 중심으로 이동해야 한다. 또한 승인을 단일 최선 모델의 선택이 아니라, 상이한 강점과 실패 양상을 가진 여러 모델을 관리하는 과정으로 이해하게 만든다. 이는 규범적 초점이 “어떤 이론이 가장 참인가?”에서 “어떤 조합과 사용 방식이 가장 덜 위험한가?”로 이동함을 의미하며, AI가 지식 생산에 깊이 관여하는 환경에서 과학자의 역할을 재규정한다. 

물론 E/K 규범은 포괄적 답변을 제공하지 않는다. 알고리즘적 편향과 불평등, 책임 귀속과 같은 윤리정치적 문제는 다루지 못했다. 또한 E/K 각 기준을 구체적인 분야에서 어떻게 수량화하고, 어떤 임계값을 승인 기준으로 설정할 것인지는 별도의 경험적, 정책적 연구를 필요로 한다. 마지막으로, 설명성을 전면적으로 포기하는 것도, 성능 지표에만 의존하는 것도 아닌 상황에서, E/K 규범이 XAI, 성능 중심 규범, 법과 규제 틀과 어떻게 결합될 수 있는지 역시 향후 과제다. 

그럼에도 불구하고, E/K 규범은 AI 비이해성 시대에 과학 이론 승인을 둘러싼 논의의 방향을 한 걸음 옮겨 놓는다. 내부 이해의 상실을 곧바로 비합리성으로 등치하지 않으면서도, 경험적 안정성과 이론적 정합성이라는 전통적 과학 가치를 포기하지 않는 외부주의적 승인 틀을 제시함으로써, “설명되지 않는 이론을 어디까지 받아들일 수 있는가”라는 질문에 대한 최소한의 규범적 답을 제공한다. 향후 과제는 이 틀을 구체적 사례와 제도 설계 논의 속에 더 깊이 이식함으로써, AI가 만들어 내는 새로운 이론들과 어떻게 함께 과학을 해 나갈 것인지에 대한 보다 정교한 지도를 그려 나가는 일일 것이다. 

## 9. 참고문헌 (References)
- Beisbart, C., & Norton, J. (2012). Why Monte Carlo simulations are inferences and not experiments. International Studies in the Philosophy of Science, 26(4), 403–422.
- Malinsky, D. (2023). Causal inference, robustness, and the epistemology of machine learning. British Journal for the Philosophy of Science.
- Jumper, J., Evans, R., Pritzel, A., et al. (2021). Highly accurate protein structure prediction with AlphaFold. Nature, 596, 583–589.
- Lakatos, I. (1978). The methodology of scientific research programmes. Cambridge University Press.
- Laudan, L. (1977). Progress and its problems. University of California Press.
- Munafò, M. R., Nosek, B. A., Bishop, D. V. M., et al. (2017). A manifesto for reproducible science. Nature Human Behaviour, 1(1), 0021.
- Popper, K. (1959/2002). The logic of scientific discovery. Routledge.
- Schmidt, M., & Lipson, H. (2009). Distilling free-form natural laws from experimental data. Science, 324(5923), 81–85.
- Szymanski, N. J., Batson, R., Liu, Y., et al. (2023). An autonomous laboratory for the accelerated synthesis of novel materials. Nature, 624, 86–91.

