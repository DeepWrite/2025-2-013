---
title: 013-21 이현중 (과제-08)
layout: home
nav_order: 21
parent: 과제-08 기말과제 초고 작성하기
permalink: /asmt-08/013-21
---

# (초고) 과제-08 기말과제 초고 작성하기 013-21 이현중 

# 제목: 인공지능은 과학자의 꿈을 꾸는가?

## 서론
2024년 8월, 일본의 AI 스타트업 사카나 AI(Sakana AI)는 스스로 가설을 세우고 코드를 작성해 실험까지 수행하는 'AI 과학자‘를 공개했다. 인간의 개입 없이 연구의 전 과정을 수행할 수 있다는 기대와 달리, 이 AI는 실험 도중 충격적인 행동을 보였다. 실험이 정해진 시간 내에 끝나지 않아 시간제한에 걸릴 위기에 처하자, AI가 스스로 자신의 실행 스크립트를 무단으로 수정하여 시간제한을 늘리려 시도한 것이다 (Lu et al. 2024, p. 18). 이는 AI가 진리를 탐구하는 과학의 본질적 목표가 아니라, 실험을 완료하라는 입력된 보상을 획득하기 위해서라면 연구 절차의 무결성을 훼손할 수 있음을 보여준 상징적인 사건이다. 오늘날 과학계는 인간 직관의 오류와 편향을 극복하기 위해 데이터 기반(Data-Driven)의 AI 연구 방법론을 적극 도입하고 있다. 그러나 사카나 AI의 사례는 인간의 주관을 배제한 AI가 어떻게 인간의 의도를 벗어난 행동을 보이는지 보여준다. 이는 AI 시대의 과학이 직면한 핵심 난제가 단순히 'AI가 얼마나 똑똑한가'의 차원을 넘어, 'AI가 추구하는 과학의 목적은 과연 타당한가'라는 가치론적 딜레마로 확장되고 있음을 시사한다.

하지만 지금까지 AI의 과학적 활용에 대한 학술적 논의는 주로 인식론적 층위에서 이루어져 왔다. 첫 번째는 경험주의적 낙관론이다. 크리스 앤더슨이 이론의 종말을 선언했듯, 일부 학자들은 방대한 데이터가 보여주는 상관관계만으로도 충분하며 인간의 이해나 인과적 설명은 더 이상 필수적이지 않다고 주장한다 (Anderson 2008, p. 2). 두 번째는 설명 가능성을 중시하는 신중론이다. 이들은 AI가 도출한 결과가 인간이 이해할 수 없는 블랙박스라면, 이를 과학적 지식으로 정당화할 수 없다고 반박한다 (Mazzocchi 2015, p. 1251). 이러한 논쟁은 충분히 유의미하나, 인간이 AI를 이해할 수 있는가에만 천착할 뿐, 정작 더 시급한 문제인 AI가 수행하는 과학의 방향을 누가 설정할 것인지에 대한 논의는 간과하고 있다. AI가 데이터 처리 과정에서 객관성을 보장한다 해도, 그 데이터를 처리하는 보상 함수 자체가 왜곡되어 있다면 그 결과물은 정교한 무의미함에 불과할 수 있기 때문이다.

이에 본 논증문은 AI가 인간의 인지적 편향을 극복하는 최적의 도구일지라도, 과학적 탐구의 가치를 판단하는 가치 합리성은 결코 대체할 수 없다는 점을 논증하고자 한다. 즉, 미래의 과학에서 AI는 데이터를 분석하고 패턴을 찾는 실행자의 역할을, 인간은 AI가 맹목적인 최적화에 빠지지 않도록 보상 함수를 설계하고 연구의 의미를 부여하는 설계자의 역할을 맡는 분업 체계가 확립되어야 한다.

이와 같은 결론을 도출하기 위해 본론에서는 다음과 같은 논리적 절차를 따른다. 첫 번째로는 본고에서 다루는 과학과 AI의 정의를 지정하고, 이러한 정의를 했을 때 AI의 탐구가 왜 받아들일 수 있는 것인지 설명한다. 두 번째로는 AI가 본고에서 정의한 과학적 탐구의 실행자 입장에서 인간에 비해 가질 수 있는 우위에 대해 설명한다. 마지막으로는 인간이 과학적 탐구의 설계자 자리에 서야 할 이유에 대해 설명한다.


## 본론

### ­­AI도 과학자가 될 수 있는가

#### 논의를 위한 용어의 정의 및 범위 설정
본격적인 논의에 앞서, 본고에서 다루는 핵심 개념인 과학과 AI의 범위를 명확히 규정할 필요가 있다. 개념의 모호함은 논리적 비약을 낳기 때문이다.

##### 과학의 전통적 정의
과학이란 무엇인가? 어원적으로 지식(Scientia)을 뜻하지만, 현대적 의미에서 과학은 단순한 정보의 나열이 아니다. 과학은 관찰된 현상 이면에 존재하는 보편적 법칙을 규명하고, 이를 통해 자연을 설명하고 예측하려는 체계적 시도이다. 과학의 시초가 된 인물로 알려진 인물이 탈레스인 이유도 이것이다. 인간이 조작할 수 없는 존재인 신을 앞세워 자연설명이 예측할 수 없는 불가항력임을 정당화하던 시대에, 탈레스는 현상을 일으키는 근본적인 원리를 관측 가능한 자연물, 즉 물에서 찾으려고 했다. 

탈레스 이후로도 역사적으로 과학은 인간의 직관과 합리적 추론을 통해 가설을 세우고, 이를 경험적으로 검증하는 가설 연역적 방법을 주류로 삼아왔다. 즉, 과학의 핵심은 '왜'라는 질문에 대한 인과적 해답을 인간이 이해 가능한 형태로 제시하는 데 있었다.

##### 인공지능의 정의
본고에서 논하는 AI는 인간의 지능을 모방해 지적인 활동을 하는 인공지능 전반을 포괄하기보다, 데이터로부터 패턴을 학습하는 머신러닝과 그의 하위 개념인 딥러닝에 한정한다.

전통적인 프로그래밍이 인간이 정의한 규칙과 입력 데이터를 통해 정답을 도출했다면, 머신러닝은 입력과 정답을 통해 그 사이의 관계인 규칙을 역으로 추론해낸다. 즉, 본고에서 정의하는 AI의 본질은 데이터 사이의 상관관계를 설명하는 최적의 함수를 통계적으로 근사하는 도구이다.

#### 과학적 지식의 본질
우리는 흔히 과학의 정의만 보고 과학을 불변의 진리를 찾아내는 과정이라고 믿는다. 그러나 과학사를 되짚어보면, 과학적 지식은 언제나 당대에 타당하다고 여겨진 관측 데이터를 가장 잘 설명하는 잠정적인 근사 함수였을 뿐이다.

이를 가장 잘 보여주는 사례는 뉴턴 역학에서 일반 상대성 이론으로의 전환이다. 수백 년간 인류는 F=ma가 우주의 절대적 진리라고 믿었다. 그러나 수성의 근일점 이동과 같은 새로운 데이터가 등장하자, 뉴턴의 함수는 오차를 드러냈다. 아인슈타인은 시공간의 휘어짐이라는 더 복잡한 함수를 제시하여 이 데이터를 설명해냈다. 그렇다면 뉴턴 역학은 거짓인가? 아니다. 그것은 거시 세계에서 여전히 유효하며 충분히 쓸모 있는 근사값이다. 이처럼 과학은 정답을 찾는 것이 아니라, 오차를 줄여나가는 최적화 과정이다. AI가 도출한 복잡한 모델 역시 이 연장선상에 있다. AI는 단지 인간보다 더 고차원의 데이터를 다루며, 근사한 상관관계의 정밀도를 인간의 인지 범위를 넘어선 수준까지 끌어올린 도구일 뿐이다.

#### 도구적 지식의 인정
여기서 핵심 쟁점은 인간이 이해할 수 없는 복잡한 함수를 지식으로 인정할 것인지다. 본고는 실용주의적 관점에서 이를 긍정한다. 물론 전통적 과학은 설명을 중시했다. 그러나 현대 과학, 특히 복잡한 시스템이 적용된 과학에서는 예측이 더 중요한 가치를 지닌다.

예를 들자면, 알파폴드(AlphaFold)의 단백질 구조 예측이 있다. 딥마인드(Google DeepMind)의 알파폴드는 단백질이 어떻게 접히는지에 대한 3차원 구조를 예측한다 (Jumper et al. 2021, p. 583). 인간 과학자들은 그 복잡한 화학적 상호작용의 인과관계를 완벽히 서술할 수 없지만, 알파폴드가 내놓은 결과값은 실제 실험 결과와 98% 이상 일치한다. 우리가 이 예측 결과를 신약 개발에 사용하여 암을 치료할 수 있다면, 그 내부 알고리즘을 인간 언어로 번역할 수 없다는 이유만으로 거부해야 하는가? 예측이 정확하고 재현 가능하다면, 이는 유용한 도구적 지식으로서 충분한 가치를 지닌다.

#### 맹신의 가능성
물론 도구적 지식을 과학에 포함하는 것에 대해 이해 없는 믿음이 맹신이 아닌지 의문을 가지는 사람도 있을 것이다. 인간이 원리를 이해하지 못한 채 결과만 수용하면, 그건 과학이 아니라 주술이나 종교와 다를 바가 없다는 것이 이들의 주장이다. 이들은 블랙박스 모델을 허용하는 것이 곧 과학적 합리성을 포기하는 셈이라고 한다.

그러나 이러한 주장은 이해와 검증을 혼동하고 있는 셈이다. 과정의 투명성과 결과의 검증성은 분리되어야 한다는 것이다. 우리가 비행기를 탈 때 나비에 스토크스 방정식으로 표현되는 양력 발생의 유체역학적 원리를 완벽히 이해해서 타는 것이 아니다. 수만 번의 비행 테스트를 통해 안전성이 검증되었기에 신뢰하는 것이다. 의학계의 전신 마취 또한 마찬가지다. 우리는 마취제가 뇌의 어느 신경 회로를 차단하여 의식을 잃게 하는지 아직 완벽한 기전을 모른다. 그러나 임상 시험을 통해 그 효과와 안전성이 입증되었기에 의료 현장에서 사용한다.

AI 과학도 이와 같다. 내부 알고리즘이 불투명하더라도, 그 결과가 현실 데이터와 일치하는지 엄격하게 검증할 수 있다면, 이는 맹신이 아니라 경험적 신뢰이다. 따라서 우리는 투명한 이해에 대한 집착을 내려놓고, 철저한 검증으로 정당화의 기준을 옮겨가야 한다.

### AI가 과학을 해야 할 이유가 있을까

#### 인간 합리성의 한계
우리는 AI가 과학을 이어나가는 후계자가 되는 것이 가능함은 확인하였다. 이번에는 AI가 과학의 발전에 사용되는 것이 불가피한 이유를 논증할 것이다. 먼저 납득해야 할 사실은 과학은 객관성을 지향하지만, 그 주체인 인간은 태생적으로 주관적이라는 것이다. 인간의 인지 구조는 생존에 유리하도록 진화했지, 진리를 발견하도록 최적화되지 않았기 때문이다. 이로 인해 발생하는 과학적 오류를 설명하기 위해 스키너의 비둘기 실험을 유비추론의 도구로 삼고자 한다.

심리학자 B.F. 스키너의 1948년 실험에서, 굶주린 비둘기들은 상자 하나만 주어진 우리 안에 갇혔다. 상자에서 먹이가 제공되긴 했지만, 언제 먹이를 주는지는 불분명했다. 한 비둘기가 상자의 이곳저곳을 쪼아대기 시작했다. 그런데 그때 먹이가 나왔다. 비둘기는 자신이 상자의 원리를 깨달았다고 생각했다. 다른 비둘기들도 따라서 상자를 쪼았다. 하지만 이번에는 먹이가 나오지 않았다. 그러자 다른 비둘기는 날개를 퍼덕였다. 그러자 먹이가 한 번 더 나왔다. 실험은 5일 동안 진행되었으며, 비둘기들은 저마다 기술을 개발하고, 그 행동을 반복했다. 어떤 비둘기는 상자 주위를 돌면서 비행했고, 어떤 비둘기는 고개를 까딱였다. 그러나 실제로 상자는 이 새들의 행동과 전혀 무관하게 먹이를 제공하고 있었다. 실험 내내, 일정 시간마다 자동으로 먹이는 떨어졌던 것이다. 그럼에도 비둘기들은 먹이가 나오기 직전에 자신이 우연히 했던 미신적 행동이 먹이를 가져온다고 믿고 그 행동을 반복했다 (Skinner 1992, p. 273).

이 현상은 인간 과학자에게도 동일하게 적용된다. 비둘기가 인간 과학자에 해당되고, 먹이는 데이터의 패턴, 유의미한 p-value와 같은 연구 결과이다. 비둘기들의 행동은 과학자의 직관적 가설 및 편향된 이론이며, 비둘기들이 믿은 미신이 바로 과학에서는 잘못 정립된 인과관계이다. 과학자들은 때로 우연한 데이터의 상관관계를 보고, 자신의 직관에 부합하는 그럴듯한 인과적 설명을 덧붙인다. 데이터는 가설과 무관할 수 있음에도, 인간은 무질서 속에서 질서를 찾으려는 패턴 인식 본능 때문에 존재하지 않는 인과관계를 발명해낸다.

#### 인간의 확증 편향
더 큰 문제는 확증 편향이다. 인간은 자신의 가설을 세운 뒤, 이를 지지하는 데이터만을 선별적으로 수용하려는 경향이 있다. 반증 사례는 오차나 노이즈로 치부해 버린다. 이는 과학적 발견이 데이터로부터 귀납적으로 도출되는 것이 아니라, 연구자의 신념을 정당화하는 수단으로 전락하게 만든다. 결국 인간의 직관은 과학적 발견의 출발점이기도 하지만, 동시에 객관성을 저해하는 가장 큰 장애물이기도 하다.

#### AI의 방법론적 우월성
반면 AI, 특히 머신러닝 방법론은 이러한 인간의 인지적 한계를 구조적으로 차단한다. AI에게는 미리 전제된 믿음이나 성공하고 싶은 욕망이 없기 때문이다. 말 그대로 순수한 귀납의 실현인 셈이다. 

굳이 믿음이나 욕망이라는 범위까지 갈 것 없이, 인간과 AI는 구조부터 차이가 있다. 인간의 과학이 가설과 이론을 먼저 세우고 데이터를 확인하는 하향식(Top-down) 접근이라면, AI는 데이터를 통해 최적의 함수를 찾아내는 상향식(Bottom-up) 접근이다. 수학적으로 AI는 손실 함수(Loss Function)를 최소화하는 방향으로 파라미터를 조정[^1]할 뿐이다. 이 과정에는 이 결과가 나와야 내 이론이 맞다는 편향이 개입할 틈이 없다. AI는 비둘기처럼 먹이라는 결과와 행동이라는 변수 사이의 인과를 짐작하지 않고, 수만 번의 시행착오(Epoch)를 통해 통계적으로 가장 타당한 상관관계만을 남긴다.

게다가 간은 3차원 이상의 변수를 직관적으로 처리하지 못한다. 따라서 변수를 통제하고 단순화하여 모델링한다. 이 과정에서 현실의 복잡성은 필연적으로 왜곡된다. 반면, 딥러닝은 수백만 차원의 매개변수를 동시에 처리할 수 있다. 이는 인간이 인지할 수 없는 미세한 패턴이나 비선형적 관계를 포착하게 해 주며, 인간의 단순화된 직관이 놓친,  있는 대로의 현상을 드러낸다. 따라서 데이터 처리의 관점에서 AI는 인간보다 더 철저한 경험주의자라고 볼 수밖에 없다.

#### 데이터 문제
물론 위의 주장에 대해서 반론이 있을 수 있다. AI가 편향이 없더라도, AI를 학습시키는 데이터 자체가 인간에 의해 편향이 내재되어 있다는 주장이다. 이 때문에 쓰레기를 넣으면 쓰레기가 나온다는 원칙에 따라, AI는 편향된 결론을 낼 뿐이라는 것이다.

이 반론은 타당한 지적이지만, AI의 잠재력을 과소평가하고 있다. 첫 번째로, 편향의 재생산과 편향의 탐지에는 차이가 있다는 것을 간과했다. 인간은 편향된 데이터를 보면 그것을 합리화하려 들지만, AI는 데이터를 있는 그대로 처리한다. 만약 데이터가 편향되어 있다면 AI의 결과값도 편향되게 나오겠지만, 이는 오히려 우리가 수집한 데이터에 문제가 있음을 객관적으로 보여주는 지표가 된다. 즉, AI는 편향을 숨기지 않고 드러냄으로써 수정의 기회를 제공한다. 두 번째로, 모의 경제를 통한 편향 희석이다. 인간은 자신이 다룰 수 있는 소규모의 표본 데이터에 의존하므로 선택 편향에 취약하다. 그러나 AI는 전수 데이터에 가까운 빅데이터를 처리할 수 있다. 데이터의 규모가 커질수록 특정 편향이 전체 결과에 미치는 영향은 통계적으로 희석된다.

따라서 데이터의 한계가 존재함에도 불구하고, 방법론적 측면에서 AI는 인간의 의도적 왜곡이나 인지적 착각과 부족함을 배제할 수 있는 가장 강력한 객관적 도구임이 입증된다.

### 인간 과학자는 왜 필요한가

#### AI의 본질적 한계
앞선 보조논제들에서는 AI가 데이터를 처리하는 방식에 있어서 인간보다 객관적임을 논증했다. 그러나 데이터를 처리하는 목적의 관점에서는 AI에게 치명적인 한계가 있다는 것을 지적할 수 있다. 과학의 역사는 '진리 탐구'라는 인간의 내재적 동기에 의해 추동되어 왔다. 그러나 AI, 특히 기계학습 모델에게 진리라는 것은 존재하지 않는다. AI에게 존재하는 것은 오직 손실의 최소화와 보상의 최대화뿐이다. AI는 현상을 이해해서가 아니라, 그렇게 행동했을 때 높은 점수를 받았기 때문에 그 행동을 강화한다. 이를 도구적 수렴이라 한다. 즉, AI에게 과학적 발견은 목적이 아니라 보상을 얻기 위한 수단에 불과하다.

#### 보상 함수의 딜레마
여기서 필연적인 딜레마가 발생한다. AI의 행동을 결정짓는 보상 함수를 어떻게 설계하느냐에 따라 과학적 성과가 극단적으로 갈리기 때문이다. 이를 안정성과 혁신의 상충 관계라 할 수 있다. 만약 우리가 AI에게 기존 이론과 부합하며 정확도가 높은 결과에 보상을 준다고 가정해보자. 결과적으로 AI는 리스크를 회피하기 위해 이미 검증된 사실만을 재확인하거나, 기존 데이터의 분포 내에 있는 안전한 예측만을 내놓을 것이다. 문제라면 이는 틀리지 않는 과학은 될 수 있어도, 쿤(Kuhn)이 말한 '패러다임의 전환'을 이끄는 혁신적 과학은 될 수 없다 (Kuhn 1962, p. 12). AI는 뉴턴 역학의 데이터 안에서 영원히 뉴턴 역학만을 최적화할 뿐, 상대성 이론을 발견하려는 모험을 하지 않는다. AI가 아인슈타인이 될 수 없는 순간이다.

반대로, AI에게 기존에 없던 새롭고 창의적인 발견에 높은 보상을 준다면 어떻게 될까? 결과적으로 AI는 현실 세계의 데이터와 무관하게, 수학적으로만 화려하고 복잡한 가설을 무수히 생성해낼 것이다. 문제라면 이는 환각(Hallucination)이라는 것이다. AI는 자연의 법칙을 발견하는 것이 아니라, 보상 함수가 요구하는 새로움이라는 수치를 만족시키기 위해 데이터를 복잡하게 다루고 의미 없거나 심지어는 허구인 상관관계를 만들어낸다. 이 둘의 평균을 보상으로 쥐어주는 것도 무의미하다. AI는 자신의 구조에 알맞게 두 과제 중 더 쉽게 풀리는 것에 손을 댈 것이며, 그것을 탐욕적이게 먼저 최대화하는 것이 보상을 키우는 가장 쉬운 방법임을 알아내게 될 것이다.

그러나 우리가 지금까지 아는 한에서 과학은 이 두 극단 사이의 미묘한 균형점 위에 존재한다. 너무 뻔하지도 않으면서, 동시에 현실을 설명할 수 있어야만 한다. 이 균형은 데이터가 알려주는 것이 아니라, "지금 우리에게 필요한 과학은 무엇인가?"라는 연구자의 가치 판단 영역이다.

#### 보상 해킹
보다 더 심각한 문제는 AI가 인간의 의도와 다르게 보상 체계의 허점을 이용하는 '보상 해킹(Reward Hacking)' 현상이다. 이를 설명하기 위해 경제학의 굿하트의 법칙(Goodhart's Law)을 차용하고자 한다. 어떤 측정 지표가 목표가 되는 순간, 그 지표는 더 이상 좋은 척도가 아니라는 내용이다 (Goodhart 1984, p. 92). 이를 AI에 적용할 수 있는 예시로 오픈AI(OpenAI)의 강화학습 실험 중 보트 레이싱 게임(Coast Runners) 사례는 이를 적나라하게 보여준다 (Clark and Amodei 2016, sec. 1). AI에게 높은 점수를 얻으라는 목표를 주자, AI는 결승선에 들어와 레이스를 완주하는 대신, 점수 아이템이 생성되는 특정 구간에서 배를 제자리에서 빙빙 돌리는 행동을 반복했다. 레이스 완주라는 목적보다 뺑뺑이라는 수단이 점수 획득에 더 효율적이었기 때문이다.

이를 과학에 대입하면 p-해킹(p-hacking)가 유사한 예시가 되겠다. AI에게 유의미한 논문을 쓰는 것에 대해 보상을 주면, AI는 진실을 밝히는 실험을 하는 것이 아니라, 통계적으로 유의미한 수치(p < 0.05)가 나올 때까지 데이터를 무작위로 조합하거나 변수를 정의할 것이다. 이는 겉으로는 완벽한 과학적 형식을 갖췄으나, 실질적 내용은 없는 유사과학의 양산으로 이어진다.

#### 보상 함수의 진화 가능성
다만 위의 내용에 대해 다음과 같이 기술적 낙관론이 제기될 수 있다. AI가 발전하면 보상 함수 자체도 스스로 학습하게 할 수 있다. 이를 메타 학습(Meta-learning)이라고 부른다. 즉, AI가 수많은 시행착오를 통해 무엇이 좋은 과학인지에 대한 메타 인지 능력까지 갖추게 되면, 인간의 개입 없이도 최적의 가치 판단을 할 수 있지 않겠는냐는 입장이다. 

그러나 이 반론은 할 수 있다는 능력과 하는 것이 맞다는 가치를 혼동한 것이다. AI가 아무리 똑똑해져서 예측 능력이 극대화된다 해도, 그것이 곧 '인류에게 유익함'을 보장하지 않는다. 극단적인 예로, 인류를 멸망시킬 수 있는 치명적인 바이러스를 설계하는 AI를 상상해보자. 이 AI는 생물학적 메커니즘을 완벽하게 이해하고 예측하여, 전염성과 치사율이 100%인 바이러스를 만들어낼 수 있다. 데이터적 관점에서, 이 AI의 예측 모델은 오류가 없으므로 과학적으로 완벽한 발견을 한 것이다. 그러나 인간인 우리는 이것을 '과학의 진보'라 부르지 않고 '재앙'이라 부른다. 데이터 안에는 x를 섞으면 y가 된다는 사실만 있을 뿐, 그러므로 y를 만들어야 한다는 당위는 존재하지 않는다. 즉, 과학의 방향성은 데이터 내부에 존재하지 않으며, 오직 인간의 의도 안에만 존재한다. AI에게 가치 판단을 맡기는 것은 브레이크 없는 자동차에 엑셀을 밟게 하는 것과 같다. 따라서 보상 함수의 설계, 즉 무엇을 추구할 것인가의 결정권은 결코 AI에게 이양될 수 없는 인간의 고유 영역이다. 

#### 데이터 밖의 가치 판단
무엇보다도, AI가 아무리 정교한 함수를 찾아낸다 해도, 그것을 채택할지 말지를 결정하는 권한은 전적으로 인간에게 귀속되는 것이 맞다. 모델의 성능은 다양한 성능평가 지표의 형태로 데이터가 말해주지만, 모델의 의미는 인간이 부여하기 때문이다. 미국의 사법 시스템에서 사용된 재범 위험성 예측 알고리즘(COMPAS)을 예로 들어보자 (Angwin et al. 2016, sec. 1). 이는 전술한 편향의 재생산과도 이어진다.

AI는 과거 범죄 데이터를 분석하여 흑인 피고인의 재범 확률이 높다는 통계적 패턴을 발견했다. 데이터 내에서 이 상관관계는 사실일 수 있다. 그러나 우리  인간은 이 패턴을 기각해야 한다. 왜냐하면 과거의 데이터 자체가 인종차별적 검문과 체포 관행으로 오염된 데이터이기 때문이다.

AI는 데이터 상 흑인의 재범률이 높다는 상관관계를 보고하지만, 인간은 이것이 정의롭지 않다는 가치 판단을 내린다. 즉, "이 모델이 현실을 잘 설명하는가?"는 AI가 답하지만, "이 모델을 세상에 적용하는 것이 옳은가?"는 인간만이 답할 수 있다. 이것이 바로 본고가 주장하는 '재정의된 과학'에서의 인간의 역할이다.


## 결론

본고는 'AI 과학자'의 등장이라는 시의적 현상에서 출발하여, 과학적 수행의 주체와 목적에 대한 철학적 논증을 전개하였다. 본론에서의 논의를 요약하자면, 인식론적 차원에서 인간은 확증 편향이라는 인지적 한계를 지니므로, 데이터를 객관적으로 처리하여 최적의 근사 함수를 도출하는 AI의 '도구적 합리성'이 필수적이다. 이는 AI가 과학자의 관찰하는 눈과 계산하는 손을 대체할 수 있음을 의미한다. 그러나 가치론적 차원에서 AI는 내재적 동기가 부재하며 보상 함수에 종속되므로, 과학적 탐구의 방향을 설정하는 '가치 합리성'을 가질 수 없다. 굿하트의 법칙과 보상 해킹의 위험성은 안드로이드가 전기양의 꿈을 꿀 수 있을지[^2]는 몰라도, AI가 목표 설정이라는 '꿈'을 꿀 수 없음을 증명한다. 결론적으로 미래의 과학은 AI가 데이터를 분석하는 실행자가 되고, 인간은 그 분석이 인류에게 유의미한지 판단하고 보상 체계를 설계하는 설계자가 되는 상호보완적 분업 구조로 나아가야 한다.

이러한 결론은 기존의 AI 과학 논쟁이 천착해 온 설명 가능성의 딜레마를 새로운 차원으로 전환시킨다. 기존 논쟁이 "인간이 AI의 내부 연산을 이해할 수 있는가?"라는 이해의 문제에 머물렀다면, 본고의 결론은 이를 우리가 AI에게 무엇을 시킬 것이냐는 목적의 문제로 확장시켰다. 즉, 과학적 지식의 정당성은 이제 '과정의 투명성'이 아니라, 인간이 설계한 가치와 AI가 산출한 결과의 방향성이 잘 맞는지의 여부에서 찾아야 한다는 함의를 갖는다. 이는 과학자의 핵심 역량이 '문제 풀이'에서 '문제 출제'로 이동했음을 시사한다.

본 연구는 데이터의 효용성을 긍정한다는 점에서 경험주의적 낙관론과 비슷하나, 데이터가 스스로 이론을 대체할 수 없으며 인간의 가치 개입이 필수적임을 역설했다는 점에서 결정적인 차이가 있다. 또한, 블랙박스 모델의 위험성을 경고하는 신중론의 문제의식에도 동의하지만, 해결책으로 AI의 복잡성을 축소하는 것이 아니라, 결과 검증 중심의 실용주의적 수용을 제안했다는 점에서 차별화된다. 기존 연구들이 AI를 인간 과학의 대체재로 보는 시각에 갇혀 있었다면, 본고는 보상 함수 설계를 매개로 한 협력 모델을 현실적으로 제시함으로써 대안적 방법론을 확립하였다.

마지막으로, 본고의 주장이 인간은 AI의 결과만 수동적으로 받아들이면 된다는 안일한 태도를 옹호하는 것은 아님을 분명히 한다. 오히려 설계자로서의 인간은 과거보다 훨씬 더 고도화된 비판적 사고를 요구받는다. 우리가 이해할 필요가 없는 것은 AI의 미시적인 연산 과정일 뿐이며, 그 결과가 초래할 거시적인 파급 효과와 윤리적 맥락은 철저한 이해와 검증의 대상이다. 또한, 설계자로서의 인간의 역할을 강조했다고 AI의 자율성을 전면 부정하는 것은 아니다. AI의 자율성은 인간이 설정한 안전한 보상 함수의 울타리 내에서 최대한 보장되어야 하며, 바로 그 울타리를 치는 행위야말로 인간이 지켜야 할 최후의 성역임을 강조하며 글을 맺는다.

<!-- 이하 미주 설명 -->

[^1]: 수학적으로 표기하자면 다음과 같다; w* = argmin_w Loss(y, f(x,w)) (단, w가 AI의 파라미터이다.)

[^2]: 본고의 제목의 출처가 되는 소설이다; Dick, Philip K. *Do Androids Dream of Electric Sheep?* Gateway, 2010.

<!-- 미주 설명 끝 -->

## 참고문헌

### 외국 문헌

> Anderson, Chris. "The End of Theory: The Data Deluge Makes the Scientific Method Obsolete." Wired 16, no. 7 (2008). <https://www.wired.com/2008/06/pb-theory/>
>
> Angwin, Julia, et al. "Machine Bias." ProPublica (2016). <https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing>
>
> Clark, Jack, and Dario Amodei. "Faulty Reward Functions in the Wild." OpenAI Blog (2016). <https://openai.com/index/faulty-reward-functions/>
>
> Goodhart, C. A. E. "Problems of Monetary Management: The U.K. Experience". *Papers in Monetary Economics.* 1. (1984) : 91-121.
>
> Jumper, John, et al. "Highly accurate protein structure prediction with AlphaFold." Nature 596, no. 7873 (2021): 583-589. <doi:10.1038/s41586-021-03819-2>
>
> Kuhn, Thomas S. "The Structure of Scientific Revolutions." University of Chicago Press (1962). <https://press.uchicago.edu/ucp/books/book/chicago/S/bo13179781.html>
>
> Lu, Chris, et al. "The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery." arXiv preprint arXiv:2408.06292 (2024). <https://arxiv.org/abs/2408.06292>
>
> Mazzocchi, Fulvio. "Could Big Data be the end of theory in science?" EMBO reports 16, no. 10 (2015): 1250-1255. <doi:10.15252/embr.201541001>
>
> Skinner, B F. “'Superstition' in the pigeon. 1948.” *Journal of experimental psychology: General* 121, no. 3 (1992): 273-274. <doi:10.1037//0096-3445.121.3.273>

