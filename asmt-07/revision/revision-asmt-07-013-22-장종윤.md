---
title: (개선) 과제-07 개인별 논증 구조 작성하기 013-22 장종윤
layout: home
nav_order: 99
parent: 013-22 장종윤 (과제-07)
permalink: /asmt-07/013-22/revision
---

# (개선) 과제-07 개인별 논증 구조 작성하기 013-22 장종윤 

## 개선 사항 메모

1. 논증 구조 변경에 따른 딜레마 해소 전략 명료화
2. 논증 구조에서 전체 논증을 AI와 인간 과학자의 유비로 대체
3. 전제1 조정 및 기존 전제 2와 전제 3의 통합, 새 전제 3 제시
4. 숨은 전제들 명시화
5. 예상반론과 재반론 변경

## 제목: AI의 설명 불가능성과 과학적 합리성: 기능적 대체 가능성 논증

## 1. 쟁점과 딜레마

| 구분 | 내용 |
|:---|:---|
| 주제(Topic) | AI가 생성한 지식을 과학적 합리성의 산물로 어디까지, 어떤 조건에서 승인할 수 있는가. |
| 도전하는 쟁점 | 블랙박스성에도 불구하고 AI 산출을 과학 지식으로 임시 수용할 수 있는 최소 요건은 무엇인가 |
| 딜레마/난제 | 설명적 합리성을 우선하면 탐색과 발견의 기회를 상실할 수 있으며, 기능적 합리성을 우선하면 설명성과 책임성이 약화될 위험이 커진다. |
| 딜레마/난제 해소/해결 방법 |지식의 생산 과정에 있어 AI는 인간 과학자와 산출물, 연구 핵심 원리, 연구 지침에서 유비적으로 상당 부분 대체 가능하다. |

① 주제(Topic): AI가 생성하는 새로운 지식이 어떤 조건에서 과학적 합리성의 산물로 승인될 수 있는지, 그리고 인간 과학자의 합리성과의 유비가 어디까지 성립하는지 논한다.

② 도전하는 학술적 쟁점: AI의 작동 원리를 충분히 설명하지 못하더라도, 경험적 검증을 통과한 산출은 임시 수용될 수 있는가?
- AI의 작동 원리나 지식 생성 과정을 이해할 수 없는 상황에서 결과의 유용성만을 근거로 AI의 합리성을 인정해야 하는지에 대한 질문이 있다. 

③ 유발되는 딜레마 또는 난제
- 딜레마 구조 (설명적 합리성 vs. 기능적 합리성의 충돌) 
    - **(A)** 설명적 합리성의 요구: AI의 작동 원리를 이해하지 못하는 상황에서 AI의 결과물을 수용하는 것은 맹목적인 신뢰이며, 오류 발생 시 책임의 주체를 명확히 할 수 없어 과학적 합리성의 원칙에 위배될 위험이 있다.
    - **(B)** 기능적 합리성의 요구: AI가 인간이 발견하지 못한 새로운 지식을 효율적으로 생성해낸다면, 그 원리를 이해할 수 없다는 이유만으로 해당 지식의 수용과 그에 따른 과학 발전의 기회를 포기하는 것은 비합리적이다.

④ 딜레마 해소 (또는 난제 해결) 전략
- 과학적 지식을 승인하는 과정은 과학적 지식을 일차적으로 생산하는 과정과 이차적으로 검증하는 과정으로 분리된다. 
- AI는 과학적 지식 생산 과정에서 인간 과학자와 동일한 합리성을 갖는다. 
    - AI가 생성해내는 지식과 인간 과학자가 도출해내는 지식에는 본질적으로 차이가 없다. 
    - AI와 인간 과학자는 지식을 생산하기 위해 각각 내부의 일관된 형식 체계와 암묵지에 의존하며, 이 둘은 기능적으로 상응하다. 
    - AI의 과학적 지식 생산은 라카토시의 과학적 연구 프로그램으로 모델링 가능하다. 
 
- 결론: 지식의 생산 과정에 있어 AI는 인간 과학자와 산출물, 연구 핵심 원리, 연구 지침에서 유비적으로 상당 부분 대체 가능하다.

## 2. 논증구조

### 기본구조

- **논제:** AI는 비이해성에도 불구하고 인간 과학자의 지식 생성의 핵심 기능을 상당 부분 대체 가능하다.

- **전제0:** 논의의 대상은 블랙박스로 인해 핵심 원리가 불투명한 AI이다. 이 AI는 지식 생산을 독립적으로 수행하거나 핵심 공정에 실질 기여할 수 있고, 설명 불가능성은 단기간 내 완전 해소되기 어렵다. 

- **전제1:** AI와 인간 과학자가 산출하는 과학적 지식은 동일 평가 규범으로 판정 가능하다. 
    - 소전제0: 지식은 진리값, 정당화 가능성, 기존 이론과의 정합성을 기준으로 분류 된다. 
        * K1: 경험적으로 참이 정당되고 기존 이론과 정합적인 지식.
        * K2: 경험적으로 참이 정당되나 기존 이론과의 정합성이 불명하며 현재는 이해되지 않는 지식.
        * K3: 진리값과 정당화 가능성이 불명인 명제.
        * K4: 경험적으로 반증된 거짓 명제.

    - 소전제1: K1은 즉시 지식으로 수용된다. K2는 설명적 합리성이 부족하더라도 기능적 합리성을 근거로 임시 수용 가능하다. (Hansson, 2025； Popper, 2002)
        * K2의 임시 수용은 지식 발견 및 탐색의 기회 상실을 방지하여 과학 발전이라는 궁극적 목표를 위한 합리적인 선택이다. 이는 설명적 합리성보다 기능적 합리성을 통한 지식 증진에 우선순위를 두는 것을 의미하며, 과학의 발전과 지식 증진이라는 궁극적 목표 하에서는 경험적 정당화가 설명적 이해보다 우위에 있다. 

    - 소전제2: 인간 과학자는 K1만 생산하는 것이 아니라 K1, K2 모두 생산한다. 
        * AI는 K2만을 생산하고 인간 과학자는 K1만을 생산하다는 이분법은 옳지 않다. 

    - 소전제3: AI가 경험적 정당화 및 반증 과정의 독립적 객관성이 보장된 환경에서 산출을 생성하면, 그 산출은 인간 과학자에 준하는 합리적인 지식 생산 기능의 성과로 인정된다.

    - 소결론: AI와 인간 과학자의 산출은 동일한 체계로 평가될 수 있다.

- **전제2:** AI의 내부 형식 체계는 지식 생산 과정에서 인간 과학자의 암묵지와 기능적으로 상응하며, 이는 합리성 승인의 유비적 토대를 제공한다.
  - 소전제1: AI의 내부 형식 체계와 인간 과학자의 암묵지는 다음의 핵심 기능 및 운영 원칙에서 상응한다. (Lu, 2025) 이 유비는 인간의 암묵지가 AI의 내부 형식 체계처럼 충분히 기능적으로 모델화될 수 있다는 가정 하에 성립한다. (Malik, 2023)
    * 1) 두 체계 모두 명제적 설명 대신 수행 규칙과 절차를 중심으로 작동한다. 
    * 2) 두 체계 모두 연습과 데이터 축적에 의해 성능이 향상되는 학습 의존적 성격을 지닌다. 
    * 3) 두 체계 모두 성능 지표를 통해 검증되고 유지된다. 
    * 4) 두 체계 모두 안전 규범과 탐색 제약 같은 명시적 규칙을 부여할 때 오작동을 줄이고 품질을 높인다. 
    * 5) 두 체계 모두 프로토콜과 로그로 절차를 외부화할 때 전이 가능성과 재현성이 향상된다. 
    * 6) 두 체계 모두 가설 생성과 실험 설계 지원 같은 지식 생성의 전단 기능을 제공한다.

  - 소전제2: 인간 쪽에서 암묵지의 명시화, 프로토콜화가 재현성과 효율을 높이듯, AI 쪽에서도 규칙, 제약의 명시화가 안전성과 일반화, 성능을 높인다. 이 때 AI의 내부 형식 체계는 설정된 규범과 프로토콜 내에서 예측 가능한 작동 일관성을 유지한다고 가정하며, 이러한 일관성이 기능적 등가성과 제도적 관리를 가능하게 하는 필수 조건이다.

  - 소전제3: AI의 비이해성으로 발생하는 책임 공백은 기능적 등가성 논증을 무효화하지 않는다. 오히려 프로토콜과 규범적 관리를 통한 제도적 보완으로 책임 소재를 객관성, 윤리적 규범을 안정적으로 유지하는 과학 공동체에 귀속시키는 방식으로 실질적이고 효과적으로 해결될 수 있으며, 이러한 제도는 AI 지식 생산의 효율성을 상쇄할 정도의 과도한 제약을 부과하지 않는다.

  - 소전제4: 논의는 지식의 1차 산출(생성, 탐색, 초기 반증 대응)에 한정되며, 이 범위에서 기능 유비가 작동한다
    * 논의가 1차 산출에 한정되는 이유는, 경험적 정당화 가능한 지식의 산출 자체가 과학적 활동의 최소한의 핵심 기능을 구성하며, 2차적 과정은 이 핵심 기능의 사후 관리적, 제도적 영역에 속하기 때문이다.

  - 소전제5: AI의 비이해성은 AI의 합리성 승인을 막는 중요한 구조적 장애물이며, 기능적 합리성으로 해소된다. 지식 생성 과정의 인과적, 심리적 메커니즘이 다르더라도, 최종 산출물과 절차의 기능적 등가성이 성립한다면 AI의 합리성 승인에 충분하다.

  - 소결론: 1차 산출 범위에서 AI의 규칙과 프로토콜 체계는 과학자의 암묵지와 동등 기능을 수행하며, 명시화와 외부화는 양쪽 모두의 성능을 상승시킨다. 

- **전제3:** AI의 과학적 지식 생산은 라카토시 ‘연구 프로그램’으로 모델링 가능하다. 
  - 소전제0: 라카토시의 연구 프로그램 모델은 핵심 가정의 보존과 주변 이론의 수정이라는 과학 발전의 일반적인 구조적 특성을 설명한다. 이 정적 프레임워크는 AI 모델의 동적 학습, 갱신 루프에도 모델링 가능하며 여러 측면에서 상응한다. (Lakatos, 1978)

  - 소전제1: AI의 지식 생산 루프는 라카토시의 하드 코어, 보호대, 휴리스틱 구조로 모델링 가능하다. 
    * 하드 코어는 목표함수, 학습규칙, 아키텍처 편향에 적용 가능하다. 이 목표 함수는 경험적 정당성 확보와 진리 탐구라는 과학적 합리성의 핵심 가치를 대변하도록 설정된다.
    * 보호대는 정규화, 증강, 하이퍼, 학습 프로토콜에 적용 가능하다. 
    * 긍정적 휴리스틱은 역전파 기반 갱신과 정보획득 중심 설계에 적용 가능하다. 
    * 부정적 휴리스틱은 핵심은 쉽게 바꾸지 않고 주변을 먼저 조정하는 원칙에 적용 가능하다. 

  - 소전제2: 프로그램의 진보와 퇴행은 신규 예측과 외삽, 일반화와 재현성의 향상 여부로 판정한다. 

  - 소전제3: 설계, 실행, 측정, 갱신의 폐루프를 통해 다음 실험을 결정하는 자율 실험 사례는, 위 연구 프로그램 모델과 운영상 정합한다. 
    * A-Lab (Szymanski, 2023)

  - 소결론: 위 대응, 판정 규칙을 채택하면 AI의 일차 산출 과정은 라카토시식 연구 프로그램으로 일관되게 기술되며, 인간 과학의 실천 규범과 유비적으로 접속된다. 

- **결론:** 따라서, 지식의 생산 과정에 있어 AI는 비이해성에도 불구하고 인간 과학자와 산출물, 연구 핵심 원리, 연구 지침에서 유비적으로 상당 부분 대체 가능하다.

### 예상반론과 재반박

- **예상반론(숨은 전제 공격):** 과학적 합리성은 단순히 지식의 산출만으로는 불충분하다. 데이터 수집, 검증, 동료 평가, 책임 소재 확정과 같은 이차적인 활동은 과학 공동체의 합리성과 제도적 정당성을 확보하기 위한 필요조건이다. AI의 블랙박스는 이 이차적 과정을 독립적으로 수행하거나 설명할 수 없으므로, 지식 생산의 핵심 기능을 상당 부분 대체했다는 주장은 논리적 비약이다.

- **재반박:**  반론은 완전한 과학 시스템의 합리성이 지녀야 할 필요조건을 요구하고 있다. 그러나 본 논증은 인간 과학자의 '지식 생성 핵심 기능'을 대체하는 것에 대한 논증이다. 경험적 정당화 가능한 지식의 산출, 암묵지와의 유비를 통해 입증된 일차적 과정의 기능적 상응은, 핵심 기능 대체라는 논제에 대한 충분조건을 충족한다. 이차적 과정은 과학의 관리적, 제도적 영역에 속하며 핵심 기능의 대체를 부정할 근거가 될 수 없다. 블랙박스라는 이유로 기능적 합리성 자체를 거부하는 것은, 과학 발전이라는 더 큰 목표를 위한 합리적 선택을 포기하는 것이다.

## 참고문헌

- Hansson, S. O. (2025). Science and pseudo-science. In E. N. Zalta & U. Nodelman (Eds.), The Stanford Encyclopedia of Philosophy (Summer 2025 ed.). Metaphysics Research Lab, Stanford University. https://plato.stanford.edu/archives/sum2025/entries/pseudo-science/  
- Popper, K. (1959/2002). The Logic of Scientific Discovery.

- Polanyi, M. (1966/2009). The Tacit Dimension. “We can know more than we can tell”
- Malik, A. (2023). Tacit knowing: What it is and why it matters. Episteme, 20(2), 349–366. https://doi.org/10.1017/epi.2021.41
- Lu, J. (2025). Tacit knowledge in large language models (GMU Working Paper in Economics No. 25-25). SSRN. https://doi.org/10.2139/ssrn.5320627
- Munafò et al. (2017). “A Manifesto for Reproducible Science.”

- Lakatos, I. (1978). The Methodology of Scientific Research Programmes.
- Laudan, L. (1977). Progress and Its Problems.
- King, R. D., et al. (2009). “The Automation of Science.”
- Jumper, J., et al. (2021). “Highly accurate protein structure prediction with AlphaFold.”
- Szymanski, N. J., et al. 2023. An autonomous laboratory for the accelerated synthesis of novel materials. Nature 624, 86–91.  

