---
title: 013-21 이현중 (과제-07)
layout: home
nav_order: 21
parent: 과제-07 개인별 논증 구조 작성하기
permalink: /asmt-07/013-21
---

# 과제-07 개인별 논증 구조 작성하기 013-21 이현중

## 제목: 인공지능은 과학자의 꿈을 꾸는가? 

## 1. 쟁점과 딜레마

| 구분 | 내용 |
|:---|:---|
| 주제(Topic) | AI가 산출한 과학적 발견이 인간의 이해나 설명 없이 지식으로서 정당화될 수 있는가 |
| 도전하려는 쟁점 | 인간의 합리적 이해 없이도 AI의 과학적 발견을 과학적 지식으로 인정할 수 있는가 |
| 딜레마/난제 | AI의 복잡성과 인간의 이해 가능성은 상충한다. 예측력을 위해 복잡성을 높이면 인간은 그 원리를 이해할 수 없어 정당화가 잘 안 되고, 설명력을 높이면 과학의 예측력을 잃는다. |
| 딜레마/난제 해소/해결 방법 | 정당화의 본질을 이해 가능성에 두되, AI의 기여를 합리성의 대체가 아닌 분업을 통한 확장으로 재구성함으로써 딜레마를 해소한다. |

① 주제(Topic): AI가 산출한 과학적 발견이 인간의 이해나 설명 없이 지식으로서의 정당화 가능성

② 도전하는 학술적 쟁점: AI의 발견이 인간의 합리적 정당화 없이도 과학적 지식으로서 인정될 수 있는가? 

- **AI의 예측 결과가 참임을 검증할 수 있더라도, 그것은 인간의 이해 불가능성을 이유로 배제되어야 하는가?**  
- **정당화란 주장하는 자가 누구이든 간에 인간이 이해 가능한 방식으로 이유가 제시되는 것인데, 그렇다면 AI의 불투명한 정당화는 지식의 조건을 충족하지 못하는가?**  
- **인간이 이해할 수 없더라도, AI가 생성한 참된 발견이 인류의 과학적 진보에 실질적으로 기여한다면, 그 유용성은 정당화의 대체물이 될 수 있는가?**

③ 유발되는 딜레마 또는 난제

- 딜레마 구조
  - **(A)** 복잡한 모델은 높은 정확도를 보일 수 있지만 인간이 이해할 수 없을 위험성이 있다. 이를 과학적 지식으로 인정할 시, 과학의 근간이 되는 자연현상의 이해가 필요없게 되어 과학의 본질에서 벗어나게 된다.
  - **(B)** 그러나 단순하고 설명 가능한 모델은 정확성이 떨어지기에, 과학의 예측력을 잃을 수 있다. 이 경우, 인간의 인지적 한계에 막혀 새로운 사실들을 찾아내기 힘들게 될 수 있다.

④ 딜레마 해소 (또는 난제 해결) 전략

- 과학적 지식의 정당화는 인간이 이해할 수 있음이 필수적으로 전제된다.
- 인간의 인지적 한계를 뛰어넘어 새로운 지식을 창출해낼 가능성이 높은 복잡한 AI는 스스로 인간의 이해라는 조건을 충족시킬 수 없다. 
- 따라서 AI의 과학적 발견은 인간의 합리성을 대체할 수 없지만, 정당화 과정의 분업화를 통해 인간의 합리성을 확장시킬 수 있다. 

## 2. 논증구조

### 기본구조

- **논제:** AI의 과학적 발견은 인간의 합리성을 대체할 수 없으며, 대신 합리성의 범위를 확장시키는 보완적 도구로 기능하도록 해야 한다.
  - **전제1:** AI의 과학적 발견의 참과 거짓은 검증 가능하나, 이는 지식의 충분조건이 아니다.
    - 지식은 전통적으로 정당화되면서 참인 믿음으로 정의된다(*The Analysis of Knowledge*, 2001).
    - AI가 산출한 결과가 실제 현상에 대해 높은 정확도를 보이더라도 그것이 왜 참인지 인간이 이해하지 못하면 정당화가 결핍된 것이다(Durán & Pozzi, 2025).
    - 복잡한 모델은 예측력은 높이지만 인간 과학자에게는 그 과정이 불투명하다(Musslick et al., 2025).
    - 다른 말로, AI는 성능 검증을 통해 참 여부는 보장하지만, 이유의 정당성은 확보하기 어렵다.
    - 따라서 AI의 결과가 참이더라도 그것이 과학적 지식으로 받아들여지기 위해서는 인간이 이해 가능한 정당화의 틀이 필요하다.
  - **전제2:** 정당화는 이해 가능성을 필수적으로 전제로 두며 인간의 합리성은 이 과정을 통해 구현된다.
    - 합리성의 정의는 이유를 제시 및 평가하고, 이를 따라 행동할 수 있는 능력이다(Kacelnik, 2006).
    - 그렇기에 합리성의 본질은 결과의 옳음이 아니라 추론 과정의 공유 가능성에 있다.
    - 그러나 AI는 자신의 결과를 믿는 주체가 아니며, 설명을 제시한다고 하더라도 인간의 언어적, 개념적 틀 안에서 번역될 때 의미를 가진다(Páez, 2019). 
    - 뿐만 아니라 AI는 명시적으로 부여한 목표를 추구할 때 목표의 의미를 이해하지 못해 의도치 않은 결과를 초래한다(Christian, 2020). 
    - 따라서 행위자의 의도 이해와 가치 해석을 전제해야 가질 수 있는 합리성은 AI가 스스로 충족할 수 없다.
  - **전제3:** AI의 발견은 인간 합리성의 대체물이 아니라 그 외연을 확장하는 보조 도구로 생각해야 한다.
    - AI는 인간의 인지적 한계를 넘어 방대한 데이터로부터 새로운 패턴을 발견할 수 있다(Musslick et al., 2025).
    - 그러나 그 의미를 해석하고 어떤 맥락으로 만드는 과정은 인간의 역할이다(Feisher, 2022).
    - 때문에 AI는 발견의 단계에서는 주도적일 수 있으나, 정당화의 단계에서는 인간의 합리성을 필요로 한다.
    - 이러한 역할 분담은 과학적 지식의 본질인 이해 가능한 설명을 유지하면서도 새로운 지식 생산의 속도와 폭을 확장한다.
    - 따라서 AI는 인간 합리성을 대체하지 않고 정당화의 분업을 통해 합리성의 외연을 넓히는 협력자의 역할을 한다.
- **결론:** AI의 과학적 발견은 인간의 합리성을 대체할 수 없다. 그러나 AI는 정당화 과정의 분업화를 통해 인간의 합리성을 확장시킬 수 있으며, 이를 통해 과학은 이해와 예측의 균형을 유지한 채 진보할 수 있다.   

### 예상반론과 재반박

- **예상반론(연역적 논증의 타당성 공격):** 전제2에서 “정당화는 반드시 인간의 이해 가능성을 전제로 한다”는 명제가 과도하게 인간 중심적이다.
  - 만약 어떤 주체가 인간보다 우수한 합리성을 가진다면, 인간의 이해 불가능성은 그 지식의 정당성을 훼손하지 않을 수 있다.

- **재반박:** 그러나 과학은 인간 공동체 내에서 공유되고 평가되는 지식 체계이다. 정당화는 이유의 존재이기도 하지만, 그 이유가 인간의 언어와 논리에 따라 소통 가능해야 함을 의미한다. 따라서 인간이 이해할 수 없는 정당화는 과학 공동체 내에서 검증 및 논의가 불가하며, 이는 과학의 공적인 성질과 합리적 토대를 무너뜨린다. AI가 새로운 지식을 발견하더라도, 그것을 과학적 지식으로 인정하기 위해서는 인간의 이해 가능한 방식으로 정당화되어야 한다.

## 참고문헌

- Musslick, S., *et al.* (2025). "Automating the practice of science: Opportunities, challenges, and implications." *Proceedings of the National Academy of Sciences of the United States of America*, 122(5), e2401238121.
- Christian, B. (2020). *The alignment problem: Machine learning and human values.* W. W. Norton & Company.
- Kacelnik, A. (2006). “Meanings of rationality.” In S. Hurley & M. Nudds (Eds.), *Rational animals?* (pp. 87-106). Oxford University Press.
- “The Analysis of Knowledge.” (2001). In Stanford Encyclopedia of Philosophy. Retrieved October 29, 2025, from https://plato.stanford.edu/entries/knowledge-analysis/
- Fleisher, W. (2022). "Understanding, Idealization, and Explainable AI." *Episteme*, 19(4), pp. 534–560.
- Durán, J.M., Pozzi, G. (2025). "Trust and Trustworthiness in AI." *Philos. Technol.* 38(16), pp. 1-31.
- Páez, A. (2019). "The Pragmatic Turn in Explainable Artificial Intelligence (XAI)", *Minds and Machines*, 29, pp. 441-459.